{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_text_links(web_link):\n",
    "    response = requests.get(web_link)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract all text from the page\n",
    "        text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "        # Print extracted text\n",
    "        return text, links\n",
    "    else:\n",
    "        return \"\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove extra spaces, newlines, and unwanted characters.\"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)  # Remove references like [1]\n",
    "    return text\n",
    "\n",
    "def filter_links(links, base_url):\n",
    "    base_domain = urlparse(base_url).netloc\n",
    "\n",
    "    # Convert relative links to absolute URLs\n",
    "    absolute_links = [urljoin(base_url, link) for link in links]\n",
    "\n",
    "    useful_links = []\n",
    "    for link in absolute_links:\n",
    "        parsed_link = urlparse(link)\n",
    "        netloc = parsed_link.netloc\n",
    "        path = parsed_link.path.lower()\n",
    "\n",
    "        # ✅ Remove external links\n",
    "        if netloc != base_domain:\n",
    "            continue\n",
    "\n",
    "        # ✅ Remove unwanted navigation and UI links\n",
    "        unwanted_keywords = [\n",
    "            \"login\", \"signup\", \"account\", \"profile\", \"settings\", \"cart\", \"terms\", \"privacy\",\n",
    "            \"help\", \"contact\", \"about\", \"faq\"\n",
    "        ]\n",
    "        if any(word in path for word in unwanted_keywords):\n",
    "            continue\n",
    "\n",
    "        # ✅ Remove JavaScript, email, and phone links\n",
    "        if link.startswith((\"javascript:\", \"mailto:\", \"tel:\")):\n",
    "            continue\n",
    "\n",
    "        # ✅ Remove pagination links\n",
    "        if \"page=\" in path or \"offset=\" in path:\n",
    "            continue\n",
    "\n",
    "        # ✅ Remove tracking, ad, and referral links\n",
    "        if any(param in link for param in [\"utm_\", \"ref=\", \"tracking\"]):\n",
    "            continue\n",
    "\n",
    "        useful_links.append(link)\n",
    "    \n",
    "    return useful_links\n",
    "\n",
    "def extract_website_content(url):\n",
    "    \"\"\"Fetch and parse content from a webpage, then convert it into a structured JSON entry.\"\"\"\n",
    "\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "        return {}, []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract title\n",
    "    title = soup.title.text if soup.title else \"No Title Found\"\n",
    "\n",
    "    # Extract main content (paragraphs)\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n",
    "    content = clean_text(\"\\n\".join(paragraphs))\n",
    "    \n",
    "    # Extract keywords (based on meta tags)\n",
    "    meta_keywords = soup.find(\"meta\", {\"name\": \"keywords\"})\n",
    "    keywords = meta_keywords[\"content\"].split(\",\") if meta_keywords else []\n",
    "    links = filter_links([a[\"href\"] for a in soup.find_all(\"a\", href=True)], url)\n",
    "\n",
    "    # Build JSON entry\n",
    "    json_entry = {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"keywords\": keywords\n",
    "    }\n",
    "    return json_entry, links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFS_links_web(base_url, visited):\n",
    "    web_dicts = []\n",
    "\n",
    "    links_to_visit = [base_url]\n",
    "\n",
    "    while len(links_to_visit)> 0:\n",
    "        url = links_to_visit.pop(0)\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "            \n",
    "        visited.add(url)\n",
    "\n",
    "        json_entry, links = extract_website_content(url)\n",
    "        if len(json_entry) == 0:\n",
    "            continue\n",
    "\n",
    "        links_to_visit.extend(links)\n",
    "\n",
    "        web_dicts.append(json_entry)\n",
    "\n",
    "    return web_dicts, visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pittsburghopera.org/\n",
      "Failed to fetch https://pittsburghopera.org/current-media-releases/PittsburghOpera_AntonyWalker_extension_2024_DRAFT.doc?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/PghProject?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/show/the-marriage-of-figaro?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/show/Nabucco?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/show/little-women?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/show/Twenty-Seven?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/show/the-rakes-progress?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/show/nabucco?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/resident-artists/2021-22-resident-artists/maire-carmack/?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/resident-artists/2021-22-resident-artists/rohan-smith-artistic-administration-assistant/?hsLang=en, status code: 404\n",
      "Failed to fetch https://pittsburghopera.org/news-and-announcements?page_num=-4656&hsLang=en, status code: 502\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.pittsburghsymphony.org/\" # 403\n",
    "url = \"https://pittsburghopera.org/\" # Yes\n",
    "url = \"https://trustarts.org/\" #403\n",
    "url = \"https://carnegiemuseums.org/\" # Yes\n",
    "url = \"https://www.heinzhistorycenter.org/\" # Yes\n",
    "url = \"https://www.thefrickpittsburgh.org/\" # Yes\n",
    "url = \"https://www.visitpittsburgh.com/events-festivals/food-festivals/\" # Yes\n",
    "url = \"https://www.picklesburgh.com/\" # Yes\n",
    "url = \"https://www.pghtacofest.com/\" # Yes\n",
    "url = \"https://pittsburghrestaurantweek.com/\"\n",
    "url = \"https://littleitalydays.com/\"\n",
    "url = \"https://bananasplitfest.com/\"\n",
    "\n",
    "visited = set()\n",
    "\n",
    "web_pages = []\n",
    "\n",
    "for url in [\"https://pittsburghopera.org/\", \"https://carnegiemuseums.org/\", \"https://www.heinzhistorycenter.org/\", \"https://www.thefrickpittsburgh.org/\", \"https://www.visitpittsburgh.com/events-festivals/food-festivals/\", \"https://www.picklesburgh.com/\"\n",
    "            \"https://www.pghtacofest.com/\", \"https://pittsburghrestaurantweek.com/\", \"https://littleitalydays.com/\", \"https://bananasplitfest.com/\"]:\n",
    "    print(url)\n",
    "    results, visited = BFS_links_web(url, visited)\n",
    "    web_pages.extend(results)\n",
    "\n",
    "# extract_website_content(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python python3",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
