{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from collections import deque\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checker if url is valid for crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_url(url):\n",
    "    \"\"\"Check if URL is valid for crawling\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this keeps only valid urls, removes external links so we stay within same domain, and removes unwanted urls like login/signup etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(links, base_url):\n",
    "    filtered_links = []\n",
    "    base_domain = urlparse(base_url).netloc\n",
    "    for link in links: \n",
    "        absolute_link = urljoin(base_url, link)\n",
    "        parsed_link = urlparse(absolute_link)\n",
    "        if not is_valid_url(absolute_link):\n",
    "            continue\n",
    "        if parsed_link.netloc != base_domain:\n",
    "            continue\n",
    "        if any(pattern in absolute_link.lower() for pattern in [\n",
    "   \n",
    "        'login', 'signin', 'signup', 'register', 'logout', 'account', 'profile',\n",
    "        'edit', 'delete', 'create', 'preferences', 'settings',\n",
    "        'mailto:', 'javascript:', 'tel:', 'sms:', '#', 'print', 'share',\n",
    "        'oc_lang=', 'lang=', 'translate=', 'setlang=', 'language=', 'translation',\n",
    "        'search', 'contact', 'feedback', 'help', 'faq',\n",
    "        'sessionid=', 'trackid=', 'utm_', 'campaign=',\n",
    "        'action=', 'do=', 'mode=', 'type=',\n",
    "        'special:', 'talk:', 'user:', 'wikipedia:', 'file:', 'mediawiki:',\n",
    "        'template:', 'help:', 'category:', 'portal:', 'draft:',\n",
    "        'index.php?', 'action=edit', 'action=history', 'oldid=', 'diff=',\n",
    "        'printable=yes', 'mobileaction=', 'title=special:', 'redlink=1',\n",
    "        'site-footer', 'footer-widgets', 'media', 'events',  'news',\n",
    "\n",
    "        ]):     \n",
    "            continue\n",
    "        filtered_links.append(absolute_link)\n",
    "    return filtered_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract text from HTML content using Beautifulsoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\"]):\n",
    "        script.extract()\n",
    "    paragraphs = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'span', 'div'])\n",
    "    text = ' '.join([p.get_text() for p in paragraphs])\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract text from PDF file using pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text + \" \"\n",
    "        return clean_text(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split text into chunks (by words) to stay within token limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=15000):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for word in words:\n",
    "        if current_length + len(word) + 1 > chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += len(word) + 1     # +1 for the space \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate qa pairs using gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - template to be used in the final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = f\"\"\"\n",
    "You are a Q&A generator. Given the text below, produce concise factual question-answer pairs in the format:\n",
    "\n",
    "Q: <question>\n",
    "A: <answer>\n",
    "\n",
    "Generate informative Q&A pairs that directly reference specific facts or information in the text. \n",
    "Make sure answers are brief and directly supported by the text.\n",
    "\n",
    "Text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa(text_chunk):\n",
    "        prompt = f\"\"\"{template}\n",
    "        Text:\n",
    "        {text_chunk}\"\"\"\n",
    "\n",
    "        api_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": \"llama3.1:8b\",\n",
    "            \"prompt\": prompt,\n",
    "            \"system\": \"You generate factual Q&A pairs from provided text.\",\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.5,\n",
    "                \"num_predict\": 1000 }}\n",
    "        \n",
    "        response = requests.post(api_url, json=payload)\n",
    "        response.raise_for_status()  \n",
    "        \n",
    "        result = response.json()\n",
    "        qa_text = result.get(\"response\", \"\").strip()\n",
    "        qa_text = re.sub(r'(\\nA:[^\\n]+)\\n(?!$)', r'\\1\\n\\n', qa_text)\n",
    "        \n",
    "        return qa_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download pdf files if they exist within a url search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, timeout=30):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        if response.status_code == 200: \n",
    "            parsed_url = urlparse(url)\n",
    "            filename = os.path.basename(parsed_url.path) or \"downloaded_file.pdf\"\n",
    "            os.makedirs(\"downloaded_files\", exist_ok=True)\n",
    "            file_path = os.path.join(\"downloaded_files\", filename)\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            return file_path\n",
    "        else:\n",
    "            print(f\"Failed to download {url}: Status code {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert domain to a safe filename (remove bad characters) & include portion of url to diffrentiate it from others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_domain_name(url):\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc\n",
    "    path = parsed.path.strip('/').replace('/', '_')    \n",
    "    safe_name = re.sub(r'[^\\w\\-]', '_', domain)\n",
    "    if path:\n",
    "        safe_name += f\"_{path[:50]}\"  #limit path length to avoid overly long filenames\n",
    "    \n",
    "    return safe_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prform BFS crawling on a seed URL\n",
    "- params:\n",
    "    - seed_url: URL to start crawling from\n",
    "    - max_depth: max depth for BFS\n",
    "    - max_pages: max number of pages to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_crawl(seed_url, max_depth=2, max_pages=20):\n",
    "    base_domain = urlparse(seed_url).netloc\n",
    "    queue = deque([(seed_url, 0)])  # url, depth\n",
    "    visited = set()\n",
    "    all_text = \"\"\n",
    "    pages_visited = 0\n",
    "    print(f\"Starting BFS on {seed_url} (max depth: {max_depth}, max pages: {max_pages})\")\n",
    "    while queue and pages_visited < max_pages:\n",
    "        current_url, depth = queue.popleft()\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        pages_visited += 1\n",
    "        print(f\"Visiting [{pages_visited}/{max_pages}]: {current_url} (depth {depth})\")\n",
    "        try:\n",
    "            response = requests.get(current_url, timeout=15)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"  Failed to fetch {current_url}: Status code {response.status_code}\")\n",
    "                continue\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
    "            #process HTML pages\n",
    "            if \"text/html\" in content_type:\n",
    "                extracted_text = extract_text_from_html(response.text)\n",
    "                all_text += \"\\n\" + extracted_text\n",
    "                \n",
    "                # contine bfs if not at max depth\n",
    "                if depth < max_depth:\n",
    "                    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                    links = [a.get(\"href\") for a in soup.find_all(\"a\", href=True)]\n",
    "                    filtered = filter_links(links, current_url)\n",
    "\n",
    "                    for link in filtered:\n",
    "                        if link not in visited:\n",
    "                            queue.append((link, depth + 1))\n",
    "            elif \"application/pdf\" in content_type:   #process pdf files --- include them in the same source as the website\n",
    "                print(f\"  Found PDF: {current_url}\")\n",
    "                pdf_path = download_file(current_url)\n",
    "                if pdf_path:\n",
    "                    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "                    all_text += \"\\n\" + pdf_text\n",
    "            #other file types - skip or handle as needed\n",
    "            else:\n",
    "                print(f\"  Skipping unsupported content type: {content_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {current_url}: {e}\")\n",
    "    print(f\"BFS completed for {seed_url} - visited {pages_visited} pages\")\n",
    "    return all_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save qa results to a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_qa_result(identifier, qa_text, output_dir=\"qa_outputs\"): \n",
    "    os.makedirs(output_dir, exist_ok=True) \n",
    "    output_file = os.path.join(output_dir, f\"QA_{identifier}.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(qa_text)\n",
    "    print(f\"Saved Q&A results to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process a single PDF file and generate qa pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_and_generate_qa(pdf_path):\n",
    "    print(f\"Processing PDF: {pdf_path}\")\n",
    "    filename = os.path.splitext(os.path.basename(pdf_path))[0] \n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if not text:\n",
    "        print(f\"  No text extracted from {pdf_path}\")\n",
    "        return \n",
    "    chunks = chunk_text(text)\n",
    "    print(f\"  Split into {len(chunks)} chunks\")\n",
    "    all_qa = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"  Generating Q&A for chunk {i+1}/{len(chunks)}\")\n",
    "        qa = generate_qa(chunk)\n",
    "        if qa:\n",
    "            all_qa.append(qa)\n",
    "    qa_text = \"\\n\\n\".join(all_qa)\n",
    "    save_qa_result(filename, qa_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process a single URL file and generate qa pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url_and_generate_qa(url, max_depth=2, max_pages=20): \n",
    "    print(f\"\\nProcessing URL: {url}\")\n",
    "    domain = safe_domain_name(url) \n",
    "    crawled_text = bfs_crawl(url, max_depth=max_depth, max_pages=max_pages)\n",
    "    if not crawled_text:\n",
    "        print(f\"  No text extracted from {url}\")\n",
    "        return \n",
    "    chunks = chunk_text(crawled_text)\n",
    "    print(f\"  Split into {len(chunks)} chunks\") \n",
    "    all_qa = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"  Generating Q&A for chunk {i+1}/{len(chunks)}\")\n",
    "        qa = generate_qa(chunk)\n",
    "        if qa:\n",
    "            all_qa.append(qa) \n",
    "    qa_text = \"\\n\\n\".join(all_qa)\n",
    "    save_qa_result(domain, qa_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Q&A for PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"qa_outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"data_sources/files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(pdf_dir):\n",
    "        pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]\n",
    "        print(f\"Found {len(pdf_files)} PDF files in {pdf_dir}\")\n",
    "        \n",
    "        for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "            process_pdf_and_generate_qa(pdf_path)\n",
    "else:\n",
    "        print(f\"Directory {pdf_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Q&A for URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"qa_outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_file = \"data_sources/urls/urls.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(urls_file):\n",
    "        with open(urls_file, \"r\") as f:\n",
    "            urls = [line.strip() for line in f if line.strip()]\n",
    "        print(f\"Found {len(urls)} seed URLs in {urls_file}\")\n",
    "        \n",
    "        for url in urls:\n",
    "            process_url_and_generate_qa(\n",
    "                url,\n",
    "                max_depth=1,  # bfs depth\n",
    "                max_pages=1  # total pages per domain \n",
    "                )\n",
    "else:\n",
    "        print(f\"URLs file {urls_file} does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
